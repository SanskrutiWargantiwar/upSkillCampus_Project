# -*- coding: utf-8 -*-
"""Traffic_Forecasting_final_CLEAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e9Acb0cc3gTEDIpaQJdko53dfhpzuGSO
"""

!pip install pandas numpy matplotlib scikit-learn statsmodels prophet tensorflow --quiet

import pandas as pd
import numpy as np

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import math

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Optional: Confirm GPU is detected
tf.config.list_physical_devices('GPU')

# Load the datasets (Make sure you uploaded them in the Files panel on the left)

df_train = pd.read_csv("train_aWnotuB.csv")
df_test = pd.read_csv("datasets_8494_11879_test_BdBKkAj.csv")

print("Train Data:")
display(df_train.head())

print("\nTest Data:")
display(df_test.head())

# Convert DateTime column to datetime type
df_train['DateTime'] = pd.to_datetime(df_train['DateTime'])
df_test['DateTime'] = pd.to_datetime(df_test['DateTime'])

# Sort the data correctly (very important for forecasting)
df_train = df_train.sort_values(['Junction', 'DateTime']).reset_index(drop=True)
df_test = df_test.sort_values(['Junction', 'DateTime']).reset_index(drop=True)

print("Preprocessing Done âœ…")

# This function converts a continuous time-series into sequence windows
# Example: If look_back = 24 â†’ use last 24 hours to predict the next hour
def create_sequences(series, look_back=24):
    X, y = [], []
    for i in range(len(series) - look_back):
        X.append(series[i:i+look_back])
        y.append(series[i+look_back])
    return np.array(X), np.array(y)


# This function builds the LSTM model structure
def build_lstm_model(input_shape):
    model = Sequential()
    model.add(LSTM(64, return_sequences=False, input_shape=input_shape))
    model.add(Dropout(0.2))   # helps prevent overfitting
    model.add(Dense(1))       # output layer â†’ predicts 1 future value
    model.compile(optimizer='adam', loss='mse')
    return model

# We will train one LSTM model per Junction (1,2,3,4)

look_back = 24       # use last 24 hours to predict next hour
epochs = 12          # you can increase later to improve accuracy
batch_size = 64
val_hours = 24 * 7   # last 7 days for validation

junctions = sorted(df_train['Junction'].unique().tolist())
lstm_models = {}     # to store model + scaler per junction
metrics = {}         # to store evaluation results

for j in junctions:
    print(f"\nðŸ”¹ Training LSTM for Junction {j} ...")

    # Select only one junction's data
    data_j = df_train[df_train['Junction'] == j]['Vehicles'].values.astype('float32').reshape(-1, 1)

    # Scale values (LSTM learns better when data is 0â€“1)
    scaler = MinMaxScaler()
    scaled = scaler.fit_transform(data_j).flatten()

    # Split into training and validation
    train_end = len(scaled) - val_hours
    train_data = scaled[:train_end]
    val_data = scaled[train_end - look_back:]

    # Create sequences
    X_train, y_train = create_sequences(train_data, look_back)
    X_val, y_val = create_sequences(val_data, look_back)

    # Reshape â†’ (samples, time steps, features)
    X_train = X_train.reshape(X_train.shape[0], look_back, 1)
    X_val = X_val.reshape(X_val.shape[0], look_back, 1)

    # Build model
    model = build_lstm_model((look_back, 1))
    es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

    # Train model
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        verbose=0,
        callbacks=[es]
    )

    # Evaluate model performance
    val_pred_scaled = model.predict(X_val).flatten()
    val_true = scaler.inverse_transform(y_val.reshape(-1,1)).flatten()
    val_pred = scaler.inverse_transform(val_pred_scaled.reshape(-1,1)).flatten()

    mae = mean_absolute_error(val_true, val_pred)
    rmse = math.sqrt(mean_squared_error(val_true, val_pred))

    metrics[j] = {"MAE": mae, "RMSE": rmse}
    lstm_models[j] = (model, scaler, scaled)

metrics

submission_rows = []

for j in junctions:
    print(f"\nðŸ”» Forecasting for Junction {j} ...")

    model, scaler, scaled_series = lstm_models[j]

    # Last 24 hours from the training data
    last_window = scaled_series[-look_back:].reshape(1, look_back, 1)

    # Number of predictions required for this junction
    test_j = df_test[df_test['Junction'] == j]
    steps = len(test_j)

    preds_scaled = []

    for _ in range(steps):
        pred = model.predict(last_window, verbose=0)[0, 0]
        preds_scaled.append(pred)

        # slide the window
        last_window = np.append(last_window[:, 1:, :], [[[pred]]], axis=1)

    # Convert predictions back to real vehicle counts
    preds = scaler.inverse_transform(np.array(preds_scaled).reshape(-1, 1)).flatten()

    # Ensure no negative predictions and round to nearest integer
    preds = np.clip(preds, 0, None)
    preds = np.round(preds).astype(int)

    # Store results with correct ID mapping
    submission_rows.append(pd.DataFrame({
        "ID": test_j["ID"].values,
        "Vehicles": preds
    }))

# Combine all junction predictions
submission = pd.concat(submission_rows).sort_values("ID").reset_index(drop=True)

submission.head()

submission.to_csv("submission_lstm.csv", index=False)
print("âœ… submission_lstm.csv file created successfully!")